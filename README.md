# WhisperBoard ğŸ™ï¸

An iOS keyboard extension that uses OpenAI Whisper locally on-device for fast, private speech-to-text transcription.

**No cloud. No subscriptions. Complete privacy.**

[![Swift Version](https://img.shields.io/badge/Swift-5.9+-orange.svg)](https://swift.org)
[![Platform](https://img.shields.io/badge/Platform-iOS%2016+-blue.svg)](https://developer.apple.com/ios/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## Features

- ğŸƒ **Local Processing** - Whisper runs entirely on-device using Core ML/MLX
- ğŸ”’ **Private by Design** - Your voice never leaves your device
- âš¡ **Real-time Streaming** - See transcription as you speak
- ğŸ¯ **Native iOS Integration** - Option A: Dictation button alongside system keyboard; Option B: Full custom keyboard
- ğŸŒ **Offline Capable** - Works without internet connection
- ğŸ“ **Smart Formatting** - Punctuation, capitalization, and voice commands

## Architecture

```
WhisperBoard/
â”œâ”€â”€ WhisperBoard/                 # Main iOS App (container)
â”‚   â”œâ”€â”€ AppDelegate.swift
â”‚   â”œâ”€â”€ SceneDelegate.swift
â”‚   â””â”€â”€ Settings/                 # App settings and model management
â”œâ”€â”€ WhisperBoardKeyboard/         # Keyboard Extension
â”‚   â”œâ”€â”€ KeyboardViewController.swift
â”‚   â”œâ”€â”€ AudioCapture.swift        # Audio recording and buffering
â”‚   â”œâ”€â”€ WhisperTranscriber.swift  # Whisper model inference
â”‚   â””â”€â”€ UI/
â”‚       â”œâ”€â”€ KeyboardView.swift    # Custom keyboard UI
â”‚       â””â”€â”€ DictationButton.swift # Microphone trigger
â”œâ”€â”€ WhisperCore/                  # Shared Core (Swift Package)
â”‚   â”œâ”€â”€ WhisperModel.swift        # Model loading and management
â”‚   â”œâ”€â”€ AudioPreprocessor.swift   # Audio â†’ Mel spectrograms
â”‚   â””â”€â”€ Tokenizer.swift           # Whisper tokenizer
â””â”€â”€ Models/                       # Core ML converted models
    â””â”€â”€ whisper-base.mlmodel      ~75MB (default)
    â””â”€â”€ whisper-small.mlmodel     ~244MB (optional)
```

## Implementation Roadmap

### Phase 1: Foundation & Setup
**Goal:** Project structure, dependencies, and basic keyboard extension

| Task | Status | Notes |
|------|--------|-------|
| 1.1 | âœ… | Create Xcode project with iOS app + Keyboard Extension |
| 1.2 | â¬œ | Configure App Groups for data sharing between app and extension |
| 1.3 | â¬œ | Set up Swift Package Manager dependencies |
| 1.4 | â¬œ | Basic keyboard UI skeleton (system keyboard fallback) |
| 1.5 | â¬œ | Request microphone permissions |

### Phase 2: Audio Pipeline
**Goal:** Capture and preprocess audio for Whisper

| Task | Status | Notes |
|------|--------|-------|
| 2.1 | â¬œ | Implement AVAudioEngine for microphone capture |
| 2.2 | â¬œ | Audio buffering (30-second sliding window) |
| 2.3 | â¬œ | Convert PCM â†’ Mel spectrograms |
| 2.4 | â¬œ | Voice Activity Detection (VAD) for auto-stop |
| 2.5 | â¬œ | Audio format normalization (16kHz, mono) |

### Phase 3: Whisper Integration
**Goal:** Convert and run Whisper models on-device

| Task | Status | Notes |
|------|--------|-------|
| 3.1 | â¬œ | Convert Whisper base model to Core ML |
| 3.2 | â¬œ | Model downloading and storage management |
| 3.3 | â¬œ | Basic inference pipeline (audio â†’ text) |
| 3.4 | â¬œ | Streaming transcription (chunked processing) |
| 3.5 | â¬œ | Post-processing (punctuation, timestamps) |

### Phase 4: Keyboard UI (Option A - Preferred)
**Goal:** Dictation button alongside iOS keyboard

| Task | Status | Notes |
|------|--------|-------|
| 4.1 | â¬œ | Research: Can we overlay on system keyboard? |
| 4.2 | â¬œ | Implement floating dictation button |
| 4.3 | â¬œ | Transcription overlay UI |
| 4.4 | â¬œ | Insert text into host app |
| 4.5 | â¬œ | Keyboard switching logic |

### Phase 5: Keyboard UI (Option B - Fallback)
**Goal:** Full custom keyboard if Option A not viable

| Task | Status | Notes |
|------|--------|-------|
| 5.1 | â¬œ | Custom QWERTY keyboard layout |
| 5.2 | â¬œ | Key press handling and haptics |
| 5.3 | â¬œ | Dictation button integration |
| 5.4 | â¬œ | Keyboard-to-keyboard switching |
| 5.5 | â¬œ | Auto-capitalization and suggestions (optional) |

### Phase 6: Polish & Optimization
**Goal:** Production-ready experience

| Task | Status | Notes |
|------|--------|-------|
| 6.1 | â¬œ | Model size optimization (quantization) |
| 6.2 | â¬œ | Battery usage optimization |
| 6.3 | â¬œ | Settings UI (model selection, language) |
| 6.4 | â¬œ | Voice commands ("period", "new line", etc.) |
| 6.5 | â¬œ | Error handling and user feedback |
| 6.6 | â¬œ | App Store preparation |

## Technical Decisions

### Why Local Whisper?
- **Privacy**: No audio sent to servers
- **Latency**: No network round-trip
- **Cost**: No API fees
- **Offline**: Works anywhere

### Model Sizes
| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| tiny | ~39MB | Fastest | Basic | Low-end devices |
| base | ~74MB | Fast | Good | **Default choice** |
| small | ~244MB | Medium | Better | High-end devices |
| medium | ~769MB | Slow | Best | Pro users (optional) |

### Core ML vs MLX
- **Core ML**: Native iOS, optimized for Neural Engine, easier deployment
- **MLX**: Apple's ML framework, potentially better performance on Apple Silicon
- **Decision**: Start with Core ML, evaluate MLX for optimization

## Getting Started

### Prerequisites
- Xcode 15+
- iOS 16+ device (simulator won't work for audio)
- Apple Developer account (for keyboard extension signing)

### Setup
```bash
# Clone the repository
git clone https://github.com/fmachta/WhisperBoard.git
cd WhisperBoard

# Open in Xcode
open WhisperBoard.xcodeproj

# Build and run on a physical device
```

### Model Setup
The app will download models on first launch, or you can pre-bundle them:
1. Download converted Core ML models
2. Add to `Models/` directory
3. Update model manifest

## Contributing

This is an open-source project! Contributions welcome:
- ğŸ› Bug reports
- ğŸ’¡ Feature requests
- ğŸ”§ Pull requests
- ğŸ“– Documentation

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) - The underlying ASR model
- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) - Inspiration for local inference
- Apple's Core ML team - For on-device ML capabilities

---

**Status:** ğŸš§ Early development - Phase 1 in progress
