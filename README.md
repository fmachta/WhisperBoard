# WhisperBoard ğŸ™ï¸

An iOS keyboard extension that uses OpenAI Whisper locally on-device for fast, private speech-to-text transcription.

**No cloud. No subscriptions. Complete privacy.**

[![Swift Version](https://img.shields.io/badge/Swift-5.9+-orange.svg)](https://swift.org)
[![Platform](https://img.shields.io/badge/Platform-iOS%2016+-blue.svg)](https://developer.apple.com/ios/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## Features

- ğŸƒ **Local Processing** - Whisper runs entirely on-device using Core ML/MLX
- ğŸ”’ **Private by Design** - Your voice never leaves your device
- âš¡ **Real-time Streaming** - See transcription as you speak
- ğŸ¯ **Native iOS Integration** - Custom keyboard with microphone button
- ğŸŒ **Offline Capable** - Works without internet connection
- ğŸ“ **Smart Formatting** - Punctuation, capitalization, and voice commands
- ğŸ‘† **Haptic Feedback** - Tactile response on key presses
- ğŸ¨ **Light/Dark Mode** - Automatic appearance adaptation

## Architecture

The app uses a **split architecture** to stay within Apple's ~50 MB keyboard extension memory limit:

- **Keyboard Extension** (~20 MB) â€“ Full QWERTY layout + voice bar, audio capture, no WhisperKit
- **Main App** â€“ WhisperKit transcription service, model management, settings
- **Communication** â€“ App Group shared container + Darwin notifications

```
WhisperBoard/
â”œâ”€â”€ WhisperBoard/Sources/
â”‚   â”œâ”€â”€ App/                      # Main app
â”‚   â”‚   â”œâ”€â”€ WhisperBoardApp.swift
â”‚   â”‚   â”œâ”€â”€ ContentView.swift
â”‚   â”‚   â””â”€â”€ TranscriptionService.swift  # Watches for keyboard audio, transcribes
â”‚   â”œâ”€â”€ KeyboardExtension/        # Keyboard extension (NO WhisperKit)
â”‚   â”‚   â”œâ”€â”€ KeyboardViewController.swift  # Full QWERTY + voice bar
â”‚   â”‚   â”œâ”€â”€ AudioCapture.swift            # AVAudioEngine recording
â”‚   â”‚   â””â”€â”€ VAD.swift                     # Voice Activity Detection
â”‚   â”œâ”€â”€ Shared/                   # Compiled into both targets
â”‚   â”‚   â””â”€â”€ SharedDefaults.swift          # App Group + Darwin notifications
â”‚   â”œâ”€â”€ WhisperKit/               # Main app only
â”‚   â”‚   â”œâ”€â”€ WhisperTranscriber.swift
â”‚   â”‚   â”œâ”€â”€ ModelManager.swift
â”‚   â”‚   â”œâ”€â”€ AudioProcessor.swift
â”‚   â”‚   â””â”€â”€ WhisperModelType.swift
â”‚   â””â”€â”€ Views/                    # Main app SwiftUI views
â”‚       â””â”€â”€ SettingsView.swift
â”œâ”€â”€ WhisperBoardTests/
â”œâ”€â”€ project.yml                   # XcodeGen configuration
â””â”€â”€ AppStore/
```

### Data Flow

```
Keyboard Extension                    Main App
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€
1. User taps mic
2. Record audio (AVAudioEngine)
3. Save PCM to App Group â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 4. Receive Darwin notification
                                     5. Load audio from App Group
                                     6. Transcribe with WhisperKit
7. Receive Darwin notification â—„â”€â”€â”€â”€ 7. Write result to App Group
8. Display text in voice bar
9. User taps "Insert"
```

## Implementation Roadmap

### Phase 1: Foundation & Setup âœ… COMPLETE
**Goal:** Project structure, dependencies, and basic keyboard extension

| Task | Status | Notes |
|------|--------|-------|
| 1.1 | âœ… | Create Xcode project with iOS app + Keyboard Extension |
| 1.2 | âœ… | Configure App Groups for data sharing between app and extension |
| 1.3 | âœ… | Set up Swift Package Manager dependencies |
| 1.4 | âœ… | Basic keyboard UI skeleton (custom keyboard layout) |
| 1.5 | âœ… | Request microphone permissions |

### Phase 2: Audio Pipeline âœ… COMPLETE
**Goal:** Capture and preprocess audio for Whisper

| Task | Status | Notes |
|------|--------|-------|
| 2.1 | âœ… | Implement AVAudioEngine for microphone capture |
| 2.2 | âœ… | Audio buffering (30-second sliding window) |
| 2.3 | âœ… | Convert PCM â†’ Mel spectrograms (via WhisperKit) |
| 2.4 | âœ… | Voice Activity Detection (VAD) for auto-stop |
| 2.5 | âœ… | Audio format normalization (16kHz, mono) |

### Phase 3: Whisper Integration âœ… COMPLETE
**Goal:** Convert and run Whisper models on-device

| Task | Status | Notes |
|------|--------|-------|
| 3.1 | âœ… | Integrate WhisperKit for model inference |
| 3.2 | âœ… | Model downloading and storage management |
| 3.3 | âœ… | Basic inference pipeline (audio â†’ text) |
| 3.4 | âœ… | Streaming transcription (chunked processing) |
| 3.5 | âœ… | Post-processing (punctuation, voice commands) |

### Phase 4: Polish & Optimization âœ… COMPLETE
**Goal:** Production-ready experience

| Task | Status | Notes |
|------|--------|-------|
| 4.1 | âœ… | Haptic feedback for key presses |
| 4.2 | âœ… | Light/dark mode support |
| 4.3 | âœ… | Memory warning handlers |
| 4.4 | âœ… | Error handling with user-friendly messages |
| 4.5 | âœ… | Comprehensive unit test coverage |
| 4.6 | âœ… | App Store preparation (assets, documentation) |
| 4.7 | âœ… | BUILD.md with build instructions |

## Phase 4 Testing & Polish Summary

### Test Coverage
- âœ… AudioProcessor Tests - Signal processing, energy computation, silence detection
- âœ… VAD Tests - Voice activity detection, state transitions
- âœ… WhisperKit Tests - Voice commands, model management
- âœ… Keyboard Tests - Keyboard optimal VAD presets
- âœ… Haptic Feedback Tests - UIImpactFeedbackGenerator tests
- âœ… Memory Warning Tests - Model unloading on memory warning
- âœ… Error Handling Tests - Localized error descriptions

### Performance Optimizations
- âœ… Memory usage target: < 150MB
- âœ… Transcription latency target: < 500ms
- âœ… Memory warning handlers implemented
- âœ… Efficient circular buffer for audio

### UI Polish
- âœ… Haptic feedback on all key presses
- âœ… Visual feedback for microphone button (pulse animation)
- âœ… Light/dark mode automatic adaptation
- âœ… Smooth animations for recording indicator

### Error Handling
- âœ… User-friendly error messages with localized descriptions
- âœ… Retry logic for model downloads
- âœ… Permission handling with graceful fallbacks

### Documentation
- âœ… README.md - Complete project documentation
- âœ… BUILD.md - Detailed build instructions
- âœ… TestingPlan.md - Comprehensive testing strategy
- âœ… App Store assets - Description, Privacy Policy, Screenshots guide

## Technical Decisions

### Why Local Whisper?
- **Privacy**: No audio sent to servers
- **Latency**: No network round-trip
- **Cost**: No API fees
- **Offline**: Works anywhere

### Model Sizes
| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| tiny | ~39MB | Fastest | Basic | Low-end devices |
| base | ~74MB | Fast | Good | **Default choice** |
| small | ~244MB | Medium | Better | High-end devices |
| medium | ~769MB | Slow | Best | Pro users (optional) |

### Core ML vs MLX
- **Core ML**: Native iOS, optimized for Neural Engine, easier deployment
- **MLX**: Apple's ML framework, potentially better performance on Apple Silicon
- **Decision**: Start with Core ML, evaluate MLX for optimization

## Getting Started

### Prerequisites
- Xcode 15+
- iOS 16+ device (simulator won't work for audio)
- Apple Developer account (for keyboard extension signing)

### Setup
```bash
# Clone the repository
git clone https://github.com/fmachta/WhisperBoard.git
cd WhisperBoard

# Open in Xcode
open WhisperBoard.xcodeproj

# Build and run on a physical device
```

### Model Setup
The app will download models on first launch, or you can pre-bundle them:
1. Download converted Core ML models
2. Add to `Models/` directory
3. Update model manifest

## Contributing

This is an open-source project! Contributions welcome:
- ğŸ› Bug reports
- ğŸ’¡ Feature requests
- ğŸ”§ Pull requests
- ğŸ“– Documentation

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) - The underlying ASR model
- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) - Inspiration for local inference
- Apple's Core ML team - For on-device ML capabilities

---

**Status:** âœ… Phase 4 Complete - App Store Ready

## Quick Start

1. **Open the project**
   ```bash
   cd WhisperBoard
   open WhisperBoard.xcodeproj
   ```

2. **Configure signing**
   - Select your development team for both targets
   - Ensure App Groups capability is enabled

3. **Build and run on device**
   - Select a physical iPhone device (simulator doesn't support audio)
   - Press âŒ˜+R to build and run

4. **Enable the keyboard**
   - Settings â†’ General â†’ Keyboard â†’ Keyboards
   - Add New Keyboard â†’ WhisperBoard
   - Grant microphone and full access permissions

## App Store Submission Checklist

- [ ] All unit tests pass (`xcodebuild test`)
- [ ] Memory usage verified < 150MB
- [ ] Transcription latency verified < 500ms
- [ ] Light/dark mode working correctly
- [ ] Haptic feedback functional
- [ ] Error messages user-friendly
- [ ] App Store screenshots created
- [ ] Privacy policy reviewed
- [ ] Build configuration set to Release

## Model Sizes

| Model | Size | Speed | Use Case |
|-------|------|-------|----------|
| Tiny | ~39MB | Fastest | Low-end devices |
| Base | ~75MB | Fast | **Default** - balanced |
| Small | ~244MB | Slower | Higher accuracy |

## Support

- **GitHub Issues**: https://github.com/fmachta/WhisperBoard/issues
- **Email**: support@whisperboard.app

## License

MIT License - See [LICENSE](LICENSE) for details.

## Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) - The underlying ASR model
- [WhisperKit](https://github.com/argmaxinc/WhisperKit) - iOS Whisper integration
- Apple's Core ML team - For on-device ML capabilities

---

**WhisperBoard** - Your voice, your text, your privacy. ğŸ™ï¸
